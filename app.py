from flask import Flask, request, jsonify
import tensorflow as tf
import joblib
import logging
from datetime import datetime, timedelta
from opencensus.ext.azure.log_exporter import AzureLogHandler
import os

# ------------------------------
# ğŸ”¹ CONFIGURATION DES LOGS AZURE
# ------------------------------
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

try:
    azure_handler = AzureLogHandler(connection_string="InstrumentationKey=47019b65-b8ca-40be-95c8-a0552c3b62b3")
    logger.addHandler(azure_handler)
    logger.info("âœ… Azure Application Insights Logging configurÃ© avec succÃ¨s.")
except Exception as e:
    print(f"âš ï¸ Erreur lors de la configuration d'Azure Application Insights Logging : {e}")

# ------------------------------
# ğŸ”¹ CHARGEMENT DU MODÃˆLE ET TOKENIZER
# ------------------------------
MODEL_PATH = "lstm_model.keras"
TOKENIZER_PATH = "tokenizer.pkl"

if not os.path.exists(MODEL_PATH):
    logger.error("âŒ Le fichier lstm_model.keras est introuvable !")
    model = None
else:
    try:
        model = tf.keras.models.load_model(MODEL_PATH)
        logger.info("âœ… ModÃ¨le 'lstm_model.keras' chargÃ© avec succÃ¨s.")
    except Exception as e:
        logger.error(f"âš ï¸ Erreur lors du chargement du modÃ¨le : {e}")
        model = None

if not os.path.exists(TOKENIZER_PATH):
    logger.error("âŒ Le fichier tokenizer.pkl est introuvable !")
    tokenizer = None
else:
    try:
        tokenizer = joblib.load(TOKENIZER_PATH)
        logger.info("âœ… Tokenizer 'tokenizer.pkl' chargÃ© avec succÃ¨s.")
    except Exception as e:
        logger.error(f"âš ï¸ Erreur lors du chargement du tokenizer : {e}")
        tokenizer = None

# ------------------------------
# ğŸ”¹ INITIALISATION DE L'APPLICATION FLASK
# ------------------------------
app = Flask(__name__)

# Stockage temporaire des erreurs de prÃ©diction
misclassified_predictions = []

@app.route("/")
def home():
    return jsonify({"message": "Bienvenue sur l'API de prÃ©diction de sentiments !"})

@app.route("/predict", methods=["POST"])
def predict():
    try:
        # VÃ©rifier si le modÃ¨le et le tokenizer sont chargÃ©s
        if model is None or tokenizer is None:
            logger.error("âŒ Le modÃ¨le ou le tokenizer n'est pas chargÃ©.")
            return jsonify({"error": "Le modÃ¨le ou le tokenizer n'est pas chargÃ©."}), 500

        # VÃ©rifier les donnÃ©es d'entrÃ©e
        data = request.json
        if "tweets" not in data:
            logger.error("âŒ Le champ 'tweets' est requis.")
            return jsonify({"error": "Le champ 'tweets' est requis."}), 400

        tweets = data["tweets"]

        if not isinstance(tweets, list):
            logger.error("âŒ Les donnÃ©es fournies ne sont pas valides.")
            return jsonify({"error": "Les donnÃ©es fournies ne sont pas valides."}), 400

        # ------------------------------
        # ğŸ”¹ PRÃ‰TRAITEMENT DES TWEETS
        # ------------------------------
        sequences = tokenizer.texts_to_sequences(tweets)
        padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=100)

        # PrÃ©diction des sentiments
        predictions = (model.predict(padded) > 0.5).astype("int32").flatten()

        results = []
        now = datetime.utcnow()
        detected_errors = []  # Stocke uniquement les erreurs pour ce batch

        # ------------------------------
        # ğŸ”¹ TEST DES PRÃ‰DICTIONS
        # ------------------------------
        expected_labels = data.get("expected_labels")  # Peut Ãªtre None

        for i, (tweet, prediction) in enumerate(zip(tweets, predictions)):
            sentiment = "positif" if prediction == 1 else "nÃ©gatif"
            result = {"tweet": tweet, "prediction": sentiment}

            if expected_labels and len(expected_labels) == len(tweets):
                expected_sentiment = "positif" if expected_labels[i] == 1 else "nÃ©gatif"
                result["expected"] = expected_sentiment

                # VÃ©rifier si la prÃ©diction est incorrecte
                if sentiment != expected_sentiment:
                    detected_errors.append({"timestamp": now, "tweet": tweet, "prediction": sentiment, "expected": expected_sentiment})
                    logger.warning(f"âš ï¸ TWEET MAL PREDIT : tweet='{tweet}', prediction='{sentiment}', attendu='{expected_sentiment}'")

            results.append(result)

        # Ajouter les erreurs dÃ©tectÃ©es Ã  la liste globale
        misclassified_predictions.extend(detected_errors)

        # Supprimer les erreurs plus vieilles que 5 minutes
        misclassified_predictions[:] = [e for e in misclassified_predictions if e["timestamp"] > now - timedelta(minutes=5)]

        # ------------------------------
        # ğŸ”¹ DÃ‰CLENCHEMENT DE L'ALERTE AZURE
        # ------------------------------
        if len(misclassified_predictions) >= 3:
            logger.error(f"ğŸš¨ ALERTE : 3 prÃ©dictions incorrectes dÃ©tectÃ©es en moins de 5 minutes ! DÃ©tails : {misclassified_predictions}")

        return jsonify({"predictions": results, "errors_detected": detected_errors})

    except Exception as e:
        logger.error(f"âŒ Erreur lors de la prÃ©diction : {e}")
        return jsonify({"error": str(e)}), 500


if __name__ == "__main__":
    logger.info("ğŸš€ Application Flask en cours d'exÃ©cution...")
<<<<<<< Updated upstream
    app.run(debug=False, host="0.0.0.0", port=5000)

=======
    app.run(debug=False, host="0.0.0.0", port=5000)
>>>>>>> Stashed changes
